{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91e9ae77",
   "metadata": {},
   "source": [
    "## Spark Merge Two DataFrames with Different Columns or Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2dce1f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"proj\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99e15f82",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://MOETD0216913.mshome.net:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>proj</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1dcd8bfb610>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe99583",
   "metadata": {},
   "source": [
    "### Creating the DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "33f998a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = [{\"Category\": 'A', \"ID\": 1, \"Value\": 121.44, \"License\": True},\n",
    "        {\"Category\": 'B', \"ID\": 2, \"Value\": 300.01, \"License\": False},\n",
    "        {\"Category\": 'C', \"ID\": 3, \"Value\": 10.99, \"License\": None},\n",
    "        {\"Category\": 'E', \"ID\": 4, \"Value\": 33.87, \"License\": True}\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b739da2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = spark.createDataFrame(data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5bf2c390",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2 = [{\"Category\": 'A', \"ID\": 5, \"Value\": 222.44, \"Age\": 37},\n",
    "        {\"Category\": 'B', \"ID\": 6, \"Value\": 500.01, \"Age\": 55},\n",
    "        {\"Category\": 'C', \"ID\": 7, \"Value\": 40.99, \"Age\": 22},\n",
    "        {\"Category\": 'E', \"ID\": 9, \"Value\": 30.87, \"Age\": 20}\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cda59997",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = spark.createDataFrame(data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4ca0ef4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StructField('Age', LongType(), True),\n",
       " StructField('Category', StringType(), True),\n",
       " StructField('ID', LongType(), True),\n",
       " StructField('Value', DoubleType(), True)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df_2.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7d382f",
   "metadata": {},
   "source": [
    "### Labelling Each DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9f99a364",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "29e47de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = df_1.withColumn('Data', F.lit('Data_1'))\n",
    "df_2 = df_2.withColumn('Data', F.lit('Data_2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edddad2a",
   "metadata": {},
   "source": [
    "### Merge using unionByName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3751a4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = df_1.unionByName(df_2, allowMissingColumns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a8704c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+-------+------+------+----+\n",
      "|Category| ID|License| Value|  Data| Age|\n",
      "+--------+---+-------+------+------+----+\n",
      "|       A|  1|   true|121.44|Data_1|null|\n",
      "|       B|  2|  false|300.01|Data_1|null|\n",
      "|       C|  3|   null| 10.99|Data_1|null|\n",
      "|       E|  4|   true| 33.87|Data_1|null|\n",
      "|       A|  5|   null|222.44|Data_2|  37|\n",
      "|       B|  6|   null|500.01|Data_2|  55|\n",
      "|       C|  7|   null| 40.99|Data_2|  22|\n",
      "|       E|  9|   null| 30.87|Data_2|  20|\n",
      "+--------+---+-------+------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "merged_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8b2fbf",
   "metadata": {},
   "source": [
    "### Doing it the old way since allowMissingColumns was only added in spark 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b08a469a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in [column for column in df_2.columns if column not in df_1.columns]:\n",
    "    df_1 = df_1.withColumn(column, F.lit(None))\n",
    "for column in [column for column in df_1.columns if column not in df_2.columns]:\n",
    "    df_2 = df_2.withColumn(column, F.lit(None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cb4c10c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Union can only be performed on tables with the compatible column types. The third column of the second table is bigint type which is not compatible with boolean at same column of first table;\n'Union false, false\n:- Project [Category#1110, cast(ID#1111L as string) AS ID#1152, License#1112, Value#1113, Data#1126, Age#1138]\n:  +- Project [Category#1110, ID#1111L, License#1112, Value#1113, Data_1 AS Data#1126, null AS Age#1138]\n:     +- LogicalRDD [Category#1110, ID#1111L, License#1112, Value#1113], false\n+- Project [cast(Age#1118L as string) AS Age#1153, Category#1119, ID#1120L, Value#1121, Data#1132, License#1145]\n   +- Project [Age#1118L, Category#1119, ID#1120L, Value#1121, Data_2 AS Data#1132, null AS License#1145]\n      +- LogicalRDD [Age#1118L, Category#1119, ID#1120L, Value#1121], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-c1443081d47c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmerged_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36munion\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m   2255\u001b[0m         \u001b[0mAlso\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstandard\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mSQL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mresolves\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0mby\u001b[0m \u001b[0mposition\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0mby\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2256\u001b[0m         \"\"\"\n\u001b[1;32m-> 2257\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2259\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    194\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Union can only be performed on tables with the compatible column types. The third column of the second table is bigint type which is not compatible with boolean at same column of first table;\n'Union false, false\n:- Project [Category#1110, cast(ID#1111L as string) AS ID#1152, License#1112, Value#1113, Data#1126, Age#1138]\n:  +- Project [Category#1110, ID#1111L, License#1112, Value#1113, Data_1 AS Data#1126, null AS Age#1138]\n:     +- LogicalRDD [Category#1110, ID#1111L, License#1112, Value#1113], false\n+- Project [cast(Age#1118L as string) AS Age#1153, Category#1119, ID#1120L, Value#1121, Data#1132, License#1145]\n   +- Project [Age#1118L, Category#1119, ID#1120L, Value#1121, Data_2 AS Data#1132, null AS License#1145]\n      +- LogicalRDD [Age#1118L, Category#1119, ID#1120L, Value#1121], false\n"
     ]
    }
   ],
   "source": [
    "merged_df = df_1.unionByName(df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5411d72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+-------+------+------+----+\n",
      "|Category| ID|License| Value|  Data| Age|\n",
      "+--------+---+-------+------+------+----+\n",
      "|       A|  1|   true|121.44|Data_1|null|\n",
      "|       B|  2|  false|300.01|Data_1|null|\n",
      "|       C|  3|   null| 10.99|Data_1|null|\n",
      "|       E|  4|   true| 33.87|Data_1|null|\n",
      "|       A|  5|   null|222.44|Data_2|  37|\n",
      "|       B|  6|   null|500.01|Data_2|  55|\n",
      "|       C|  7|   null| 40.99|Data_2|  22|\n",
      "|       E|  9|   null| 30.87|Data_2|  20|\n",
      "+--------+---+-------+------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "merged_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
